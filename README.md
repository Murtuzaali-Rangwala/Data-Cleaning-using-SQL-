# Data Cleaning using SQL
In this project, I utilized a dataset sourced from Kaggle and performed data cleaning tasks using MySQL Workbench. Data cleaning is a crucial step in the data analysis pipeline, particularly given the growing importance of high-quality, accurate data in industries today. 

Clean data ensures more reliable insights and enhances the decision-making process, a trend that is becoming increasingly critical in the age of big data.

Steps Taken in the Data Cleaning Process:

  1. Remove Duplicates
       Duplicate records can skew analysis and lead to inaccurate conclusions. Identifying and eliminating duplicates ensures the integrity of the dataset.
  
  2. Standardizing the Data
       Standardization involves transforming data into a consistent format, making it easier to analyze and compare. This is essential for improving the efficiency of data queries and enhancing the accuracy of models.
  
  3. Handling Null Values
       Missing or null values can lead to incomplete analysis or errors in data processing. I used appropriate methods to handle null values based on their significance and context.
  
  4. Removing Irrelevant Columns
       Irrelevant or redundant columns are removed to reduce complexity and enhance data processing performance. This step streamlines the dataset, making it more focused and aligned with the analysis goals.
